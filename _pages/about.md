---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am currently an Associate Professor at University of Science and Technology Beijing, China (åŒ—äº¬ç§‘æŠ€å¤§å­¦ï¼Œè®¡ç®—æœºä¸é€šä¿¡å·¥ç¨‹å­¦é™¢). Prior to that I worked as a research fellow at National University of Singapore (NUS), Singapore, supervised by Prof. [Haizhou Li (ææµ·æ´²)](https://colips.org/~eleliha/). I received my Ph.D. degree from Queen Mary, University of London (QMUL), U.K., under the supervision of Prof. [Andrea Cavallaro](http://www.eecs.qmul.ac.uk/~andrea/). During my Ph.D. degree, I went to Fondazione Bruno Kessler [(FBK)](https://www.fbk.eu/en/), Trento, Italy, as a research assistant, supervised by Dr. [Maurizio Omologo](https://www.amazon.science/author/maurizio-omologo) and Dr. [Alessio Brutti](https://ict.fbk.eu/people/detail/alessio-brutti-fbk-speech-processing/). I received my B.Eng. and M.Sc. degrees both from the University of Edinburgh, U.K., supervised by Prof. [James Hopgood](https://www.eng.ed.ac.uk/about/people/dr-james-r-hopgood).

My research interest mainly focuses on audio-visual fusion, includes speech processing, speaker localization and tracking, active speaker detection, gesture synthesis, automatic speech recognition. I have published more than 30 papers at the top-tiered international AI journals/conferences such as TMM, TASLP, TII, ACM MM, ICRA ICASSP, INTERSPEECH.

ğŸ‰ğŸ‰ Our lab in USTB is actively looking for research assistants and postgraduate students. Please contact me at **qianxy@ustb.edu.cn** for more details. å¼€å±•**ä»¥æ·±åº¦å­¦ä¹ ä¸ºæ ¸å¿ƒçš„è¯­éŸ³ä¿¡å·å¤„ç†ã€è§†è§‰+å¬è§‰å¤šæ¨¡æ€äººæœºäº¤äº’**ç ”ç©¶ï¼Œå­¦ç”Ÿå¯ä»¥æ ¹æ®å…´è¶£è‡ªç”±é€‰æ‹© 

<font color=red> ã€è¯¾é¢˜ç»„ç»è´¹å……è¶³ï¼Œç§‘ç ”æ°›å›´æµ“åšï¼Œç°æ‹›æ”¶2026å¹´å…¥å­¦è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ç¡•å£«ç ”ç©¶ç”Ÿã€åšå£«ç ”ç©¶ç”Ÿï¼Œä¹Ÿæ¬¢è¿ä¼˜ç§€çš„æœ¬ç§‘ç”ŸåŠ å…¥ã€‘</font>
æ¬¢è¿è®¡ç®—æœºåŸºç¡€è¾ƒå¥½ï¼Œæœ‰ç¨‹åºè®¾è®¡ç«èµ›æˆ–è€…ç§‘ç ”ç»å†ï¼Œæœ‰å¿—äºæ”»è¯»ç¡•å£«/åšå£«ç ”ç©¶ç”Ÿå’Œå‡ºå›½æ·±é€ çš„åŒå­¦è”ç³»æˆ‘ (é™„CVåŠè‡ªæˆ‘ä»‹ç»ï¼Œqianxy@ustb.edu.cn)
ä½ å°†è·å¾—ï¼š
- å‚åŠ å›½å†…/å›½é™…ä¼šè®®çš„æœºä¼š
- æµ·å†…å¤–åæ ¡å¯¼å¸ˆè”åˆæŒ‡å¯¼
- æœ‰æœºä¼šæ¨èåˆ°è‹±å›½çˆ±ä¸å ¡å¤§å­¦ã€è¨é‡Œå¤§å­¦ã€ä¼¦æ•¦ç›ä¸½å¥³ç‹å¤§å­¦ã€é¦™æ¸¯ç§‘æŠ€å¤§å­¦ã€é¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆæ·±åœ³ï¼‰ã€æ–°åŠ å¡å›½ç«‹å¤§å­¦ç­‰å­¦ä¹ è®¿é—®

ğŸ”¥ News
- 2025.07	å…¥é€‰åŒ—äº¬å¸‚â€œé«˜åˆ›è®¡åˆ’â€é’å¹´æ‰˜ä¸¾äººæ‰
- 2025.07	è·è¯„åŒ—äº¬å›¾åƒå›¾åƒå­¦ä¼šæœ€ç¾å¥³ç§‘æŠ€å·¥ä½œè€…
- 2025.03   one JSTSP paper acceptedï¼
- 2025.01   two TASLP paper accepted!
- 2024.12   one AAAI paper accepted!
- 2024.09   one ICASSP paper accepted!
- 2024.07   two ACM MM paper accepted!
- 2024.05   one IJSR paper accepted!
- 2024.04   two INTERSPEECH paper accepted!
- 2024.02   one PRL paper accepted!

 
# ğŸ“œ Research Area
<table style="border-collapse: collapse; border: none;">
  <tr style="border: none;">
    <td style="border: none;"> <font color="#0b5394"> Speech Processing </font>: <BR>&nbsp;&nbsp; 
      Speaker recognition and verification è¯´è¯äººè¯†åˆ«ï¼›Speech separation and extraction è¯­éŸ³åˆ†ç¦»ï¼›Key-word spotting å…³é”®è¯æ£€æµ‹ï¼›
    Automatic Speech Recognition è¯­éŸ³è¯†åˆ«</td>
    <td style="border: none;"> <font color="#0b5394"> Computer Vision </font>: <BR>&nbsp;&nbsp; Face detection and recognition äººè„¸æ£€æµ‹åŠè¯†åˆ«; Lip reading å”‡è¯»ï¼›Gesture synthesis å§¿æ€ç”Ÿæˆ</td>
  </tr>
  <tr style="border: none;">
    <td style="border: none;"> <font color="#0b5394"> Multi-modal Processing </font>: <BR>&nbsp;&nbsp; Audio-visual active speaker detection è¯´è¯äººæ´»è·ƒæ£€æµ‹; Text-to-speech Synthesis è¯­éŸ³åˆæˆï¼›Speaker Localization and Tracking å£°æºå®šä½åŠè¿½è¸ª</td>
    <td style="border: none;"> <font color="#0b5394"> Self-supervised Learning </font>: <BR>&nbsp;&nbsp; Self-supervised speech processing è‡ªç›‘ç£å­¦ä¹  </td>
  </tr>
</table>

# ğŸ’» Research Experiences
- *2022.10 - Present*, Associate Professor, University of Science and Technology of Beijing (USTB), Beijing, China.
- *2022.03 - 2022.09*, Visiting Scholar, Chinese University of Hong Kong (CUHKSZ), Shenzhen, China.
- *2020.02 - 2022.02*, Research Fellow, National University of Singapore (NUS), Singapore.
- *2017.04 - 2018.12*, Research Asistant, Fondazione Bruno Kessler (FBK), Trento, Italy.
- *2014.06 - 2014.08*, Research Asistant, Heriot-Watt University (HWU), Edinburgh, United Kingdom.

# ğŸ“– Educations
- *2015.11 - 2019.11*, Ph.D. in Computer Scicence, Queen Mary, University of London (QMUL), London, U.K. 
- *2014.08 - 2015.08*, M.Sc. in Signal Processing and Communications, University of Edinburgh (UoE), U.K.  (Distinctionï¼Œå“è¶Š) 
- *2012.09 - 2014.06*, B.Eng. in Electronics and Electrical Engineering, University of Edinburgh (UoE), U.K. (First Class Honorsï¼Œä¸€ç­‰è£èª‰) 
- *2010.09 - 2012.06*, B.Eng. in Information Engineering, Nanjing University of Aeronautics and Astronautics (NUAA), Nanjing, China. (Top: 3%)


# ğŸ“ Selected Publications 

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

**Kaiming He**, Xiangyu Zhang, Shaoqing Ren, Jian Sun

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div> -->
<!-- # ğŸ“ Representative Publications
 -- **Representative Works** --
- **Xinyuan Qian**, Zhengdong Wang, Jiadong Wang, Guohui Guan, Haizhou Li, [Audio-Visual Cross-Attention Network for Robotic Speaker Tracking](https://ieeexplore.ieee.org/document/9968308)ï¼Œ**IEEE/ACM Transactions on Audio, Speech and Language Processing**, 2022.
- **Xinyuan Qian**, Alessio Brutti, Oswald Lanz, Maurizio Omologo, Andrea Cavallaro, [Audio-visual tracking of concurrent speakers](https://ieeexplore.ieee.org/document/9362311), **IEEE Transactions on Multimedia**,2021.
- **Xinyuan Qian**, Alessio Brutti, Oswald Lanz, Maurizio Omologo, Andrea Cavallaro, [Multi-speaker tracking from an audioâ€“visual sensing device](https://ieeexplore.ieee.org/document/8656587), **IEEE Transactions on Multimedia**, 2019.
- **Xinyuan Qian**, Qi Liu, Jiadong Wang, Haizhou Li, [Three-Dimensional Speaker Localization: Audio-Refined Visual Scaling Factor Estimation](https://ieeexplore.ieee.org/document/9466446), **IEEE Signal Processing Letters**, 2021.
- **Xinyuan Qian**, Qiquan Zhang, Guohui Guan and Wei Xue, [Deep Audio-visual Beamforming for Speaker Localization](https://ieeexplore.ieee.org/document/9750883), **IEEE Signal Processing Letters**, 2022.
- **Xinyuan Qian**, Bidisha Sharma, Amine El Abridi, Haizhou Li, [SLoClas: A Database for Joint Sound Localization and Classification](https://arxiv.org/abs/2108.02539), **COCOSDA**, 2021, **Best Paper Award**.
- **Xinyuan Qian**, Maulik Madhavi, Zexu Pan, Jiadong Wang, Haizhou Li, [Multi-target DoA estimation with an audio-visual fusion mechanism](https://ieeexplore.ieee.org/document/9413776), **ICASSP**, 2021.
- **Xinyuan Qian**, Alessio Xompero, Alessio Brutti, Oswald Lanz, Maurizio Omologo, Andrea Cavallaro, [3D mouth tracking from a compact microphone array co-located with a camera](https://ieeexplore.ieee.org/document/8461323), **ICASSP**ï¼Œ2018.
- **Xinyuan Qian**, Alessio Brutti,  Maurizio Omologo, Andrea Cavallaro, [3D audio-visual speaker tracking with an adaptive particle filter](https://ieeexplore.ieee.org/abstract/document/7952686), **ICASSP**ï¼Œ2017.
- **Xinyuan Qian**, Jichen Yang, Alessio Brutti, [Speaker Front-back Disambiguity using Multi-channel Speech Signals](https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ell2.12666?af=R), **Electronics Letters** 2022.
 -->

-- **2025** --
- **Xinyuan Qian**, Jiaran Gao, Yaodan Zhang, Qiquan Zhang, Hexin Liu, Leibny Paola Garcia, Haizhou Li, [SAV-SE: Scene-aware Audio-Visual Speech Enhancement with Selective State Space Model](https://arxiv.org/pdf/2411.07751), **JSTSP**, 2025
- Tianhao Zhang, Jiawei Zhang, Jun wang, **Xinyuan Qian<sup>`*`</sup>**, Xucheng Yin, [FaceSpeak: Expressive and High-Quality Speech Synthesis from Human Portraits of Different Styles](https://arxiv.org/pdf/2501.03181), **AAAI**, 2025
- Wei Zhang, Tian-Hao Zhang, Chao Luo, Hui Zhou, Chao Yang, **Xinyuan Qian<sup>`*`</sup>**, Xu-Cheng Yin, [Breaking Through the Spike: Spike Window Decoding for Accelerated and Precise Automatic Speech Recognition](https://arxiv.org/pdf/2501.03257), **ICASSP**, 2025
- Ruijie Tao, **Xinyuan Qian**, Yidi Jiang, Junjie Li, Jiadong Wang, Haizhou Li, [Audio-Visual Target Speaker Extraction with Selective Auditory Attention](https://ieeexplore.ieee.org/abstract/document/10835186), **TASLP**, 2025
- Jiadong Wang, **Xinyuan Qian<sup>`*`</sup>**, Haizhou Li, [Audio-Visual Target Speaker Extraction with Selective Auditory Attention](https://arxiv.org/pdf/2209.01768), **TASLP**, 2025

-- **2024** --
- **Xinyuan Qian**, Xianghu Yue, Jiadong Wang, Huiping Zhuang, Haizhou Li, [Analytic Class Incremental Learning for Sound Source Localization with Privacy Protection](https://arxiv.org/pdf/2409.07224), **SPL**, 2025
- Miao Liu, Jing Wang, **Xinyuan Qian**, Haizhou Li, [RListenFormer: Responsive Listening Head Generation with Generated Listening HGeeandesrated Listening Heads Non-autoregressive Transformers], **ACM MM**, 2024
- Xianghu Yue, Xueyi Zhang, Yiming Chen, Chengwei Zhang, Mingrui Lao, Huiping Zhuang, **Xinyuan Qian<sup>`*`</sup>**, Haizhou Li, [MMAL: Multi-Modal Analytic Learning for Exemplar-Free Audio-Visual Class Incremental Tasks], **ACM MM**, 2024
- **Xinyuan Qian**, Jingkai Xu, Yuxuan Gao, Minshu Li, Wanlin Li, Xu-Cheng Yin, [Understanding Dynamic Auditory Perception for
Water Filling Level Estimation], **IJSR**, 2024
- **Xinyuan Qian**, Hao Tang, Jichen Yang, Hongxu Zhu, Xu-Cheng Yin, [Dual-Path Transformer-Based GAN for Co-speech Gesture Synthesis](https://link.springer.com/article/10.1007/s12369-024-01136-y), **IJSR**, 2024
- Yan Liu; Li_fang Wei; **Xinyuan Qian<sup>`*`</sup>**; Tianhao Zhang; Songlu Chen; Xucheng Yin, [M3TTS: Multi-Modal Text-to-Speech of Multi-Scale Style Control for Dubbing](https://elsevier-ssrn-document-store-prod.s3.amazonaws.com/prletters/c88b9fd1-51a4-460e-a720-2ef628f60be8-meca.pdf?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjENn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIFMqyV8AhJXpSjKDL2wwqhxKsquUvfQKAT9ATxs53bhCAiACa%2BB6HtyTHcDRuUE1NiefCvwDrzYQIFe3zhl%2Br2tE4SrGBQix%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAQaDDMwODQ3NTMwMTI1NyIMqGMe2eGr8ErOIPD9KpoF6gCT7eO4kMNbY5KFy5TF0eivao%2F9t4VOQ%2BQYPFL2xdIQd2hg0xe5l6M4sq4RS10fuwhJXItyA%2FtPuE%2BAXznaAFDrNEGP0uK78Qyem5TxGC3WQYRlVyGBcGm%2F4ebqtcEx1nxn%2F3wj2XqeY0WN67zfJAwMfVwpn%2FcrlGHESCtflJUZcViLybwovoreAx582GqfrqxX7rIr9PyrDGUKmW1D2mfIY94Sr5v2b0lzUAe%2BILdv5B0BKIGBEZ6NBjAmo85%2F1KYoJ43PY4D2zAhAZYxM0o%2BbL7I14wGgMWW822PWF1gWjTLJEN3QG2U3r8m34%2BScn0qx0oEC%2BZoWSGEmwymVOt5yzv8mWefYV2pA8mSmWBrdM9j5ts%2Bch2O%2FSbibyZFgjZiCdNPjBu29uOZq2reAnFkqBTU1fdealNkU4x4kRzQ5BRJi6EXtUCqcOlRXtxTgzq014FGvD1T2dEYeZCR%2BtcBjoyvuNDb9vb%2BjehTzT3wwD72Ryex38efR4LqmaytawnTQIdZH5OHgsWpl4x6mQpw8XLi0zBLgfBiW1Ti6cSgZxYoqSv%2FTgYdecIi%2BVupxr3HbqDYdyV6T0xP0v61aGr8QJo0OzhujH1qZErJlGzo%2Fal0PKDIq8c82%2FqeCwnb3zYq%2FujQvzrzx4jPKVy4k71kvfbofzjak5bhbIml1xZA9UuWHR1wwreEnEGOJvMy5oc6Te5kNfYyQY99jmMIzwcnNM6kx1YZyfvaIweP8u21dTbBhMZXByOMvB139AF8I5OX98BHMeHqpQ70FgsQ9luRM9Qh%2FT3IR%2FzCeWlfyLrtGBjCy4q6QZ%2Fs3wCu1rpJ3P%2FfVAn09lof7GknmI%2BFFyNLlj0cZywuDx%2BFCzGgqz10Kes6x8znJdoA6MN2vhrUGOrIBmHZ9fOQJux11cpSTU1yMmpqcjzicsBoYZu0KcaH5GCnnMX0TSnaEgjxyMHLU%2FU8Zc5iqgldXeMY9N5fyY%2FIIDEUOsZX2%2FXKGRf0iTXFlTF2MrqYzOF9aWBwDItU5NQLfLugsYgT3%2F%2BfZu1es27jZ9ouEjOnxQxKivDzSO7j6fCEOG6f4FGASIH0gkSHv9yguGiSQCOwyIR3h4LwIo%2FWnI4YQJJd6SO0PHLBtFvgRm2eB7g%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240725T010847Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAUPUUPRWE6VPFLJLM%2F20240725%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=94bbbea5716ea603623bc3eb37d6f60a81764820acf8b558b7a9b574f2b4e1ab), **PRL**, 2024
- Miao Liu, Jing Wang, **Xinyuan Qian**, Xiang Xie, [Visually Guided Binaural Audio Generation with Cross-Modal Consistency](https://ieeexplore.ieee.org/abstract/document/10446399/), **PRL**, 2024
- Yu Chen, **Xinyuan Qian<sup>`*`</sup>**, Zexu Pan, Kainan Chen, Haizhou Li, [LocSelect: Target Speaker Localization with an Auditory Selective Hearing Mechanism](https://arxiv.org/pdf/2310.10497.pdf), **ICASSP**, 2024
- **Xinyuan Qian**, Zexu Pan, Qiquan Zhang, Kainan Chen, Shoufeng Lin, [GLMB 3D speaker tracking with video-assisted multi-channel audio optimization functions](https://ieeexplore.ieee.org/abstract/document/10446460), **ICASSP**, 2024

-- **2023** --
- Miao Liu, Jing Wang, **Xinyuan Qian**, Haizhou Li, [Audio-Visual Temporal Forgery Detection Using Embedding-Level Fusion and Multi-Dimensional Contrastive Loss](https://ieeexplore.ieee.org/abstract/document/10290956), *TCSVT*, 2023
- **Xinyuan Qian**, Wei Xue, Qiquan Zhang, Ruijie Tao, Yiming Wang, Kainan Chen, Haizhou Li, [Bi-directional Image-Speech Retrieval Through Geometric Consistency]ï¼Œ **ICCVW**, 2023
- **Xinyuan Qian**, Wei Xue, Qiquan Zhang, Ruijie Tao, Haizhou Li, [Deep Cross-modal Retrieval Between Spatial Image
and Acoustic Speech](https://ieeexplore.ieee.org/document/10285477)ï¼Œ **TMM**, 2023
- Tian-Hao Zhang, Hai-Bo Qin, Zhi-Hao Lai, Song-Lu Chen, Qi Liu, Feng Chen, **Xinyuan Qian<sup>`*`</sup>**, Xu-Cheng Yin [Rethinking Speech Recognition with A Multimodal Perspective via Acoustic and Semantic Cooperative Decoding](https://arxiv.org/abs/2305.14049), **INTERSPEECH**, 2023
- Longting Xu, Jichen Yang<sup>`*`</sup>, Chang Huai You, **Xinyuan Qian<sup>`*`</sup>**, Daiyu Huang, [Device Features Based on Linear Transformation With Parallel Training Data for Replay Speech Detection](https://ieeexplore.ieee.org/abstract/document/10103148), **TASLP**, 2023.
- Jiadong Wang, **Xinyuan Qian<sup>`*`</sup>**, Malu Zhang, Robby T Tan, Haizhou Li, [Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert](https://arxiv.org/pdf/2303.17480.pdf), **CVPR**, 2023.
- Tian-Hao Zhang, Qi Liu, **Xinyuan Qian<sup>`*`</sup>**, Song-Lu Chen, Feng Chen, Xu-Cheng Yin<sup>`*`</sup>, [Self-Convolution for Automatic Speech Recognition](https://ieeexplore.ieee.org/document/10095330/), **ICASSP**, 2023.
- Moran Chen, Qiquan Zhang, Qi Song, **Xinyuan Qian**, Ruijin Guo, Mingjiang Wang, Deying Chen [Neural-Free Attention for Monaural Speech Enhancement Towards Voice User Interface for Consumer Electronics](https://ieeexplore.ieee.org/abstract/document/10070570), **TCE**, 2023.
- Kaspar Althoefer, Yonggen Ling, Wanlin Li, **Xinyuan Qian**, Wang Wei Lee, Peng Qi, [A Miniaturised Camera-based Multi-Modal Tactile Sensor](https://arxiv.org/pdf/2303.03093.pdf), **ICRA**, 2023.


-- **2022** --
- **Xinyuan Qian**, Zhengdong Wang, Jiadong Wang, Guohui Guan, Haizhou Li, [Audio-Visual Cross-Attention Network for Robotic Speaker Tracking](https://ieeexplore.ieee.org/document/9968308)ï¼Œ**TASLP**, 2022.
- **Xinyuan Qian**, Qiquan Zhang, Guohui Guan and Wei Xue, [Deep Audio-visual Beamforming for Speaker Localization](https://ieeexplore.ieee.org/document/9750883), **SPL**, 2022.
- **Xinyuan Qian**, Jichen Yang, Alessio Brutti, [Speaker Front-back Disambiguity using Multi-channel Speech Signals](https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ell2.12666?af=R), **Electronics Letters** 2022.
- Zexu Pan, **Xinyuan Qian<sup>`*`</sup>**, Haizhou Li, [Speaker Extraction with Co-Speech Gestures Cue](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774925), **SPL**, 2022.
- Qiquan Zhang, **Xinyuan Qian<sup>`*`</sup>**, Zhaoheng Ni, Aaron Nicolson, Eliathamby Ambikairajah, Haizhou Li, [TFA-SE: A Time-Frequency Attention Module for Neural Speech Enhancement](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9966661), **TASLP**, 2022.
- Hongxu Zhu, Qiquan Zhang, Peng Gao, **Xinyuan Qian**, [Speech-Oriented Sparse Attention Denoising for Voice User Interface Toward Industry 5.0](https://ieeexplore.ieee.org/abstract/document/9893339/), **TII**, 2022.
- Yanjie Fu, Meng Ge, Haoran Yin, **Xinyuan Qian**, Longbiao Wang, Gaoyan Zhang, Jianwu Dang, [Iterative Sound Source Localization for Unknown Number of Sources](https://arxiv.org/abs/2206.12273), **TNTERSPEECH**, 2022.

-- **2021** --
- **Xinyuan Qian**, Alessio Brutti, Oswald Lanz, Maurizio Omologo, Andrea Cavallaro, [Audio-visual tracking of concurrent speakers](https://ieeexplore.ieee.org/document/9362311), **TMM**,2021.
- **Xinyuan Qian**, Qi Liu, Jiadong Wang, Haizhou Li, [Three-Dimensional Speaker Localization: Audio-Refined Visual Scaling Factor Estimation](https://ieeexplore.ieee.org/document/9466446), **SPL**, 2021.
- **Xinyuan Qian**, Bidisha Sharma, Amine El Abridi, Haizhou Li, [SLoClas: A Database for Joint Sound Localization and Classification](https://arxiv.org/abs/2108.02539), **COCOSDA**, 2021, **Best Paper Award**.
- **Xinyuan Qian**, Maulik Madhavi, Zexu Pan, Jiadong Wang, Haizhou Li, [Multi-target DoA estimation with an audio-visual fusion mechanism](https://ieeexplore.ieee.org/document/9413776), **ICASSP**, 2021.
- Jiadong Wang, **Xinyuan Qian<sup>`*`</sup>**, Zihan Pan, Malu Zhang, Haizhou Li, [GCC-PHAT with Speech-oriented Attention for Robotic Sound Source Localization](https://ieeexplore.ieee.org/document/9561885), **ICRA**, 2021.
- Ruijie Tao, Zexu Pan, Rohan Kumar Das, **Xinyuan Qian**, Mike Zheng Shou, Haizhou Li, [Is Someone Speaking? Exploring Long-term Temporal Features for Audio-visual Active Speaker Detection](https://arxiv.org/abs/2107.06592), **ACM MM**, 2021.

-- **2020 and Before** --
- Shoufeng Lin<sup>`#`</sup>, **Xinyuan Qian<sup>`#`</sup>**, [Audio-Visual Multi-Speaker Tracking Based on the GLMB Framework](https://www.isca-speech.org/archive_v0/Interspeech_2020/pdfs/1969.pdf), **INTERSPEECH**, 2020.
- **Xinyuan Qian**, Alessio Brutti, Oswald Lanz, Maurizio Omologo, Andrea Cavallaro, [Multi-speaker tracking from an audioâ€“visual sensing device](https://ieeexplore.ieee.org/document/8656587), **TMM**, 2019.
- **Xinyuan Qian**, Alessio Xompero, Alessio Brutti, Oswald Lanz, Maurizio Omologo, Andrea Cavallaro, [3D mouth tracking from a compact microphone array co-located with a camera](https://ieeexplore.ieee.org/document/8461323), **ICASSP**ï¼Œ2018.
- Oswald Lanz, Alessio Brutti, Alessio Xompero, **Xinyuan Qian**, Maurizio Omologo, Andrea Cavallaro, [Accurate Target Annotation in 3D from Multimodal Streams](https://ieeexplore.ieee.org/document/8682619), **ICASSP**ï¼Œ2018.
- **Xinyuan Qian**, Alessio Brutti,  Maurizio Omologo, Andrea Cavallaro, [3D audio-visual speaker tracking with an adaptive particle filter](https://ieeexplore.ieee.org/abstract/document/7952686), **ICASSP**ï¼Œ2017.
- Deepayan Bhowmik, Andrew Wallace, Robert Stewart, **Xinyuan Qian**, Greg Michaelson, [Profile driven dataflow optimisation of mean shift visual tracking](https://ieeexplore.ieee.org/document/7032066), **GlobalSIP**ï¼Œ2014. 

# ğŸ– Certifications and Awards
- Outstanding Undergraduate Mentorï¼ŒUSTB, 2024
- Best Student Paper Award, INTERSPEECH, 2024
- Best Paper Award, COCOSDA, 2021
- The 3rd place winner in the ActivityNet Challenge (Speaker), CVPR Workshop, 2021
- Outstanding international research associatant,  FBK, Trento, Italy, 2019
- Full Ph.D. scholarship in QMUL, London, U.K., 2015-2019
- Outstanding Youth Female Research Engineer Scholarship, Edinburgh, U.K., 2014
- Excellent international student scholarship, Edinburgh, U.K., 2013-2014
- Shanghai 801 scholarship, 2011
- First-Class Scholarship Award, NUAA, Nanjing, China, 2010-2011

# ğŸ’¬ Teaching 
- ç¦»æ•£æ•°å­¦
- æ¨¡å¼è¯†åˆ«åŸºç¡€

# ğŸ‘” Projects
- Young Scientists Fund of the National Natural Science Foundation of China å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘é’å¹´é¡¹ç›® (PI), 2023
- CCF-Tencent AI-Lab Open Fund è…¾è®¯ AI LabçŠ€ç‰›é¸Ÿä¸“é¡¹ï¼ˆPI), 2023
- Fundamental Research Funds for the Central Universities ä¸­å¤®é«˜æ ¡åŸºæœ¬ç§‘ç ”ä¸šåŠ¡ç»è´¹ ï¼ˆPI), 2023
- Eigenspace Audio Technology Project (PI), 2023
- Beijing Municipal Natural Science Foundation - Xiaomi åŒ—äº¬å¸‚è‡ªç„¶ç§‘å­¦åŸºé‡‘â€”â€”å°ç±³åˆ›æ–°è”åˆåŸºé‡‘é¡¹ç›® (Co-PI), 2023

- Human Robot Interaction Project-Phase 1, Singapore, 2020-2022
- Huawei Research&Design Project, Shenzhen, China, 2022
- Shenzhen Research Institute of Big Data Internal Project (SRIBD), Shenzhen, China, 2022

# ğŸ’¬ Reviewer
- Reviewer of TASLP, TMM, Neural Networks, ICASSP, INTERSPEECH, SPL, ICPR



<!-- Google Analytics -->
<script async src="https://catherine-qian.github.io/"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'GA_MEASUREMENT_ID');
</script>
<!-- End Google Analytics -->




